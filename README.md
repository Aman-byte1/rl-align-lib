# RL Align Lib

A comprehensive library for Reinforcement Learning from Human Feedback (RLHF) and direct preference alignment.

## Features
- **Online Policy Methods**: PPO, GRPO, TRPO, REINFORCE++
- **Offline Preference Methods**: DPO, IPO, KTO, ORPO
- **Specialized Reasoning Methods**: GDPO, GSPO, SDPO, RLVR

## Installation
```bash
pip install -e .
```

## Usage
Detailed documentation coming soon.
